# Job 2 for set1 - used in dev and staging targets
# Using YAML anchors to avoid duplication

job-config: &job-config
  set1-job-2:
    name: "Set1 Job 2 - ${var.environment}"
    tasks:
      - task_key: "process_data"
        notebook_task:
          notebook_path: ../../src/notebook.py
        new_cluster:
          spark_version: "13.3.x-scala2.12"
          node_type_id: "Standard_DS3_v2"
          num_workers: 1
    max_concurrent_runs: 1
    timeout_seconds: 3600
    schedule:
      quartz_cron_expression: "0 0 12 * * ?"
      timezone_id: "UTC"

targets:
  dev:
    resources:
      jobs: 
        <<: *job-config
  staging:
    resources:
      jobs: 
        <<: *job-config