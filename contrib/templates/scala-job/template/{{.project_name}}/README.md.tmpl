# {{.project_name}}

The '{{.project_name}}' project was generated by using the scala-job template.

## Getting started

1. Install the Databricks CLI from https://docs.databricks.com/dev-tools/cli/install.html. The version must be v0.226.0 or later.

2. Authenticate to your Databricks workspace (if you have not done so already):
    ```
    $ databricks configure
    ```

3. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a job called
    `[dev yourname] {{.project_name}}_job` to your workspace.
    You can find that job by opening your workspace and clicking on **Workflows**.

4. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```

5. To run a job, use the "run" command:
   ```
   $ databricks bundle run
   ```

6. Optionally, install developer tools such as the Databricks extension for Visual Studio Code from
   https://docs.databricks.com/dev-tools/vscode-ext.html.

7. For documentation on the Databricks Asset Bundles format used
   for this project, and for CI/CD configuration, see
   https://docs.databricks.com/dev-tools/bundles/index.html.

## Local Devloop

### Prerequisites
- sbt v1.10.2 or later
- java 17

1. Import the current directory in your ide (we recommend IntelliJ) where build.sbt is located. Verify it is imported as sbt project.
2. If you donâ€™t have java, in Intellij, go to File -> Project Structure, SDKs -> + sign to add 17 -> OK

   Then Run -> Edit Configurations -> Set version to Java 17 from drop
3. You should now be able to run the code with the UI but you can also just simply run `sbt run` in the terminal.

## Customizations

### Job configuration
The bundles piggybacks off the same configurations used in APIs. If you want to use an existing cluster instead of spinning one up everytime, replace job_cluster_key in tasks with existing_cluster_id: <your_cluster_id>

You can also change to an all-purpose (dedicated) cluster by removing the data_security_mode of the created cluster

